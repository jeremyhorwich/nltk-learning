Using this as a catch all repository for any various NLP learning projects that make use of or are based off of principles from the NLTK library.

# 1. complexityDetection.py:

- Created this as a wrap-up to my reading of this page: https://www.nltk.org/book/ch08.html. I intentionally did not use the NLTK library for this as I wanted to test my understanding of concepts behind library functions discussed in the aforementioned link.
- Originally the goal of this algorithm was to be able to detect if a sentence had ambiguity without constructing the full sentence. However, I quickly realized that ambiguity is a meaningless term without the construction of a full sentence as a prerequisite - in other words, saying a phrase is ambiguous doesn't make sense unless you know the sentence can be fully parsed successfully at least two different ways
- Modifying WFST would work very well, but using that would be too obvious for it to be of any real value to me as an exercise
- Ultimately I didn't want to simply abandon the work so I refactored into a less ambitious algorithm which detects if a sentence is anything more than extremely simple from a syntactic standpoint. It uses a bottom up approach similar to shift-reduce parsing
- There is absolutely no tokenization included whatsoever beyond the very crude separating words by whitespace
- Test cases: "I shot an elephant" should return False. "I shot an elephant in my pajamas" should return True.

# 2. posTagging.py:

- Created as a wrap-up to my readings of these pages: https://www.nltk.org/book/ch05.html and https://web.stanford.edu/~jurafsky/slp3/3.pdf.
- The above links discuss N Grams and their uses in predictive functions, including POS tagging. One facet I didn't understand about the theory from the resources above is why in POS tagging models which leverage N Grams for pedictions, these models don't simply abstract out to the parts of speech themselves and make predictions based on the patterns of parts of speech independent of the specific tokens underlying them. The goal of this algorithm was therefore twofold; to (a) in general, to implement a rudimentary backoff algorithm to tag parts of speech based on NLTK's Brown Corpus, and (b) in particular, to abstract the tokens out to their parts of speech as much as possible and discover how effective that might be.
- The algorithm is what Jurafsky and Martin (second link) describe as "stupid backoff", that is to say, it doesn't implement any kind of smoothing in the predictions. Implementing Kneser-Ney smoothing is a candidate for a future learning project. In model training, we take the most common words (how many most common is determined by user input, which I'll call most common threshold), map them to direct POS definitions, as well as POS bigrams and trigrams. When we make predictions, we use these definitions in the following order, moving to the next when we can't find a mapping in our model: First, the direct definitions; then, trigrams (making predictions on the next token's POS based on the previous two tokens' POS); bigrams; and finally falling back on the most common POS in the training corpus.
- The model also includes import/export of trained models, and spits out an accuracy of our prediction based on what tags are actually provided for the tokens in the corpus.
- Running the completed algorithm, I disovered why abstraction isn't used. Take for example the following image:
![Screenshot 2024-03-03 184702](https://github.com/jeremyhorwich/nltkLearning/assets/71720955/4711232e-b4b7-4bb1-ac7a-511f47176ea6)

- This is the dictionary variable for the bigrams in a model with a most common threshold size of 1000, captured in VS Code debugging in a breakpoint in the tagging function. There are a couple things to note here. First, there aren't that many unique parts of speech, so our bigrams number very few. This is an example of bad precision/recall tradeoff. But the even bigger problem is the recursive nature of some of these definitions. What do I mean by that? Take our prediction for CONJ. Most often in our corpus, X follows CONJ. Yet also, most often CONJ follows X. So until we happen to find a trigram or direct definition that better suits our needs, the model will predict CONJ X CONJ X CONJ X ad nauseum, which clearly can't be accurate. This is even more apparent in the case of a POS most often following itself, in which case our model might predict a dozen or so NOUNs in a row. Ultimately, the model had the highest accuracy when bigrams and trigrams weren't used, and when it simply used either a direct definition taken from the corpus and the most common part of speech when none was available. 
- There a couple of solutions I can think of to the above. First off, rework the algorithm to predict parts of speech following specific words, as laid out in the NLTK handbook. Second, you could also implement some kind of repetition detector, so that the predictor knows to fall back on the next backoff solution when the prediction begins to repeat itself - but I believe this second solution to be a clumsy one. 
